# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö
–§–ò–ü–°, –†–æ—Å–ø–∞—Ç–µ–Ω—Ç, Linkmark, WIPO, EUIPO
"""

import re
import time
import json
import urllib.parse
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
import requests
from bs4 import BeautifulSoup
import Levenshtein
from transliterate import translit, get_available_language_codes

from config import TRADEMARK_RESOURCES, APP_CONFIG, API_KEYS
from models import TrademarkCheckResult, RiskLevel


class TextSimilarity:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤"""

    @staticmethod
    def normalize_text(text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not text:
            return ""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower().strip()
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s]', '', text)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        return text

    @staticmethod
    def levenshtein_similarity(text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω—É (0-1)"""
        text1 = TextSimilarity.normalize_text(text1)
        text2 = TextSimilarity.normalize_text(text2)

        if not text1 or not text2:
            return 0.0

        distance = Levenshtein.distance(text1, text2)
        max_len = max(len(text1), len(text2))

        if max_len == 0:
            return 1.0

        return 1 - (distance / max_len)

    @staticmethod
    def contains_similarity(text1: str, text2: str) -> float:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –¥—Ä—É–≥–æ–π"""
        text1 = TextSimilarity.normalize_text(text1)
        text2 = TextSimilarity.normalize_text(text2)

        if not text1 or not text2:
            return 0.0

        if text1 in text2 or text2 in text1:
            return 1.0

        return 0.0

    # –°–ª–æ–≤–∞—Ä—å —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–π —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ (—Å–æ–∑–≤—É—á–∏–µ)
    PHONETIC_MAP_RU_TO_EN = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }

    PHONETIC_MAP_EN_TO_RU = {
        'a': '–∞', 'b': '–±', 'c': '–∫', 'd': '–¥', 'e': '–µ', 'f': '—Ñ', 'g': '–≥',
        'h': '—Ö', 'i': '–∏', 'j': '–¥–∂', 'k': '–∫', 'l': '–ª', 'm': '–º', 'n': '–Ω',
        'o': '–æ', 'p': '–ø', 'q': '–∫', 'r': '—Ä', 's': '—Å', 't': '—Ç', 'u': '—É',
        'v': '–≤', 'w': '–≤', 'x': '–∫—Å', 'y': '–π', 'z': '–∑',
        'ch': '—á', 'sh': '—à', 'th': '—Ç', 'ph': '—Ñ', 'ck': '–∫'
    }

    @staticmethod
    def transliterate_variants(text: str) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–∑–≤—É—á–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª + —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—é –≤ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã + —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.
        """
        variants = [text]
        text_lower = text.lower()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
        has_cyrillic = any('\u0400' <= c <= '\u04FF' for c in text)
        has_latin = any('a' <= c.lower() <= 'z' for c in text)

        # 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É
        try:
            translit_ru = translit(text, 'ru', reversed=True)
            if translit_ru and translit_ru != text:
                variants.append(translit_ru)
        except:
            pass

        try:
            translit_en = translit(text, 'ru')
            if translit_en and translit_en != text:
                variants.append(translit_en)
        except:
            pass

        # 2. –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è (–¥–ª—è —Å–æ–∑–≤—É—á–∏—è)
        if has_cyrillic:
            # –†—É—Å—Å–∫–∏–π ‚Üí –õ–∞—Ç–∏–Ω–∏—Ü–∞ (—Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏)
            phonetic_en = ""
            for char in text_lower:
                phonetic_en += TextSimilarity.PHONETIC_MAP_RU_TO_EN.get(char, char)
            if phonetic_en and phonetic_en != text_lower:
                variants.append(phonetic_en)
                variants.append(phonetic_en.capitalize())

        if has_latin:
            # –õ–∞—Ç–∏–Ω–∏—Ü–∞ ‚Üí –†—É—Å—Å–∫–∏–π (—Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏)
            phonetic_ru = text_lower
            # –°–Ω–∞—á–∞–ª–∞ –∑–∞–º–µ–Ω—è–µ–º –¥–∏–≥—Ä–∞—Ñ—ã
            for digraph, ru_char in sorted(TextSimilarity.PHONETIC_MAP_EN_TO_RU.items(),
                                           key=lambda x: -len(x[0])):
                if len(digraph) > 1:
                    phonetic_ru = phonetic_ru.replace(digraph, ru_char)
            # –ü–æ—Ç–æ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã
            result = ""
            for char in phonetic_ru:
                if char in TextSimilarity.PHONETIC_MAP_EN_TO_RU and len(char) == 1:
                    result += TextSimilarity.PHONETIC_MAP_EN_TO_RU[char]
                else:
                    result += char
            phonetic_ru = result
            if phonetic_ru and phonetic_ru != text_lower:
                variants.append(phonetic_ru)
                variants.append(phonetic_ru.capitalize())

        # 3. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø–∏—Å–∞–Ω–∏—è (—á–∞—Å—Ç—ã–µ –∑–∞–º–µ–Ω—ã)
        alternatives = {
            'c': 'k', 'k': 'c',  # c/k –≤–∑–∞–∏–º–æ–∑–∞–º–µ–Ω—è–µ–º—ã
            'i': 'y', 'y': 'i',  # i/y –≤–∑–∞–∏–º–æ–∑–∞–º–µ–Ω—è–µ–º—ã
            'ph': 'f', 'f': 'ph',
            'ks': 'x', 'x': 'ks',
        }
        for old, new in alternatives.items():
            if old in text_lower:
                variants.append(text_lower.replace(old, new))

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ
        return list(set(v for v in variants if v and v.strip()))

    @staticmethod
    def check_similarity(text1: str, text2: str,
                         threshold: float = 0.8) -> Tuple[bool, float, str]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤

        Returns:
            (is_similar, score, reason)
        """
        norm1 = TextSimilarity.normalize_text(text1)
        norm2 = TextSimilarity.normalize_text(text2)

        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if norm1 == norm2:
            return True, 1.0, "–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è - –æ—Ü–µ–Ω–∫–∞ –ó–ê–í–ò–°–ò–¢ –æ—Ç —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–ª–∏–Ω
        if norm1 in norm2 or norm2 in norm1:
            shorter = min(len(norm1), len(norm2))
            longer = max(len(norm1), len(norm2))
            # –ß–µ–º –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ, —Ç–µ–º –º–µ–Ω—å—à–µ –æ—Ü–µ–Ω–∫–∞
            containment_score = shorter / longer
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –æ—á–µ–Ω—å —Ä–∞–∑–Ω—ã–µ –ø–æ –¥–ª–∏–Ω–µ - —ç—Ç–æ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if containment_score < 0.7:
                return True, containment_score, f"–ß–∞—Å—Ç–∏—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ ({containment_score:.0%})"
            else:
                return True, 0.9, "–û–¥–∏–Ω —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–º"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω—É
        lev_score = TextSimilarity.levenshtein_similarity(text1, text2)
        if lev_score >= threshold:
            return True, lev_score, f"–°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω—É: {lev_score:.2f}"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
        variants1 = TextSimilarity.transliterate_variants(text1)
        variants2 = TextSimilarity.transliterate_variants(text2)

        best_translit_score = 0
        for v1 in variants1:
            for v2 in variants2:
                v1_norm = TextSimilarity.normalize_text(v1)
                v2_norm = TextSimilarity.normalize_text(v2)

                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
                if v1_norm == v2_norm:
                    return True, 1.0, "–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è)"

                # –í—Ö–æ–∂–¥–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
                if v1_norm in v2_norm or v2_norm in v1_norm:
                    shorter = min(len(v1_norm), len(v2_norm))
                    longer = max(len(v1_norm), len(v2_norm))
                    containment_score = shorter / longer
                    if containment_score > best_translit_score:
                        best_translit_score = containment_score

                score = TextSimilarity.levenshtein_similarity(v1, v2)
                if score > best_translit_score:
                    best_translit_score = score

        if best_translit_score >= threshold:
            return True, best_translit_score, f"–°—Ö–æ–∂–µ—Å—Ç—å —Å —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–µ–π: {best_translit_score:.2f}"

        return False, max(lev_score, best_translit_score), "–ù–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"


class TrademarkChecker:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.similarity_threshold = APP_CONFIG["text_similarity_threshold"]

    def check_trademark(self, text: str, mktu_classes: List[int] = None) -> TrademarkCheckResult:
        """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏ - –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ –ø–æ–¥–∫–ª–∞—Å—Å–∞—Ö"""
        raise NotImplementedError


class RospatentPlatformChecker(TrademarkChecker):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞
    https://searchplatform.rospatent.gov.ru/patsearch/v0.2/

    –í–ê–ñ–ù–û: –≠—Ç–æ—Ç API –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    –î–ª—è –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:
    https://searchplatform.rospatent.gov.ru/trademarks

    –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API: –û—Ç–∫—Ä—ã—Ç—ã–µ_API_–ò–°_–ü–ü.docx
    """

    def __init__(self):
        super().__init__()
        self.base_url = "https://searchplatform.rospatent.gov.ru"
        self.api_url = f"{self.base_url}/patsearch/v0.2"
        self.search_url = f"{self.api_url}/search"
        self.tm_search_url = f"{self.base_url}/trademarks"  # –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –¢–ó
        self.resource_info = TRADEMARK_RESOURCES["rospatent_platform"]

        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —á–µ—Ä–µ–∑ Bearer —Ç–æ–∫–µ–Ω
        api_key = API_KEYS.get("rospatent", "")
        if api_key:
            self.session.headers.update({
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            })
            self.api_available = True
        else:
            self.api_available = False

    def check_trademark(self, text: str, mktu_classes: List[int] = None) -> TrademarkCheckResult:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –ø–∞—Ç–µ–Ω—Ç–Ω—ã–π API –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.

        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: API –∏—â–µ—Ç –ø–æ –ø–∞—Ç–µ–Ω—Ç–∞–º, –∞ –Ω–µ –ø–æ —Ç–æ–≤–∞—Ä–Ω—ã–º –∑–Ω–∞–∫–∞–º –Ω–∞–ø—Ä—è–º—É—é.
        –î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¢–ó —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.
        """
        result = TrademarkCheckResult(
            resource_name=self.resource_info["name"],
            resource_url=self.tm_search_url,  # –°—Å—ã–ª–∫–∞ –Ω–∞ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¢–ó
            search_query=text,
            mktu_classes=mktu_classes or []
        )

        if not self.api_available:
            result.notes = f"API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¢–ó –≤—Ä—É—á–Ω—É—é: {self.tm_search_url}"
            result.status = RiskLevel.YELLOW
            return result

        try:
            # –ü–∞—Ç–µ–Ω—Ç–Ω—ã–π –ø–æ–∏—Å–∫ - –Ω–∞—Ö–æ–¥–∏—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –ø–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            search_body = {
                "qn": text,
                "limit": 20,
                "offset": 0,
                "sort": "relevance"
            }

            # –í—ã–ø–æ–ª–Ω—è–µ–º POST –∑–∞–ø—Ä–æ—Å –∫ API (—É–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–æ 60 —Å–µ–∫)
            response = self.session.post(
                self.search_url,
                json=search_body,
                timeout=60
            )

            if response.status_code == 200:
                data = response.json()
                self._process_search_results(result, data, text)
            elif response.status_code == 401:
                result.notes = "–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á."
                result.status = RiskLevel.YELLOW
            elif response.status_code == 403:
                result.notes = "–î–æ—Å—Ç—É–ø –∫ API –∑–∞–ø—Ä–µ—â—ë–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞."
                result.status = RiskLevel.YELLOW
            else:
                result.notes = f"–û—à–∏–±–∫–∞ API ({response.status_code}). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞."
                result.status = RiskLevel.YELLOW

        except requests.exceptions.RequestException as e:
            result.notes = f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API: {str(e)}. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞."
            result.status = RiskLevel.YELLOW
        except Exception as e:
            result.notes = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            result.status = RiskLevel.YELLOW

        return result

    def _process_search_results(self, result: TrademarkCheckResult,
                                 data: Dict, search_text: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ API –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞"""
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        total = data.get("total", 0)
        hits = data.get("hits", [])

        if total == 0:
            result.status = RiskLevel.GREEN
            result.notes = "–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –±–∞–∑–µ –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            return

        for hit in hits:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–∞ API
            snippet = hit.get("snippet", {})

            # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö
            trademark_text = (
                snippet.get("title", "") or
                snippet.get("name", "") or
                hit.get("id", "")
            )

            # –ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
            reg_number = snippet.get("registration_number", "") or snippet.get("reg_number", "")

            # –ö–ª–∞—Å—Å—ã –ú–ö–¢–£
            tm_classes = snippet.get("index_class", [])
            if isinstance(tm_classes, str):
                tm_classes = [tm_classes]

            # –°—Ç–∞—Ç—É—Å —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞
            tm_status = snippet.get("status", "")

            # –ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å
            holder = snippet.get("holder", "") or snippet.get("applicant", "")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            is_similar, score, reason = TextSimilarity.check_similarity(
                search_text, trademark_text, self.similarity_threshold
            )

            if is_similar:
                result.found_matches.append({
                    "text": trademark_text,
                    "registration_number": reg_number,
                    "similarity_score": score,
                    "reason": reason,
                    "classes": tm_classes,
                    "status": tm_status,
                    "holder": holder,
                    "dataset": hit.get("dataset", ""),
                    "doc_id": hit.get("id", "")
                })

                if score == 1.0:
                    result.exact_match = True
                else:
                    result.similar_match = True

                if reg_number:
                    result.registration_numbers.append(reg_number)
                result.similarity_score = max(result.similarity_score, score)

        # –í–ê–ñ–ù–û: –≠—Ç–æ –ø–∞—Ç–µ–Ω—Ç–Ω—ã–π –ø–æ–∏—Å–∫, –Ω–µ –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤!
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Å—è—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä
        # –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¢–ó –∏–¥—ë—Ç —á–µ—Ä–µ–∑ Linkmark

        result.status = RiskLevel.GREEN  # –ü–∞—Ç–µ–Ω—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å—Ç–∞—Ç—É—Å –¢–ó
        result.found_matches = []  # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–∞—Ç–µ–Ω—Ç—ã –∫–∞–∫ –¢–ó

        if total > 0:
            result.notes = f"–ü–∞—Ç–µ–Ω—Ç—ã: {total} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π. –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¢–ó —Å–º. Linkmark."
        else:
            result.notes = "–í –ø–∞—Ç–µ–Ω—Ç–Ω–æ–π –±–∞–∑–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

    def get_document_details(self, doc_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –ø–æ ID"""
        if not self.api_available:
            return None

        try:
            url = f"{self.api_url}/docs/{doc_id}"
            response = self.session.get(url, timeout=30)
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return None

    def similar_image_search(self, image_path: str) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤"""
        if not self.api_available:
            return None

        try:
            url = f"{self.api_url}/similar_search"
            with open(image_path, 'rb') as f:
                files = {'file': f}
                # –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –Ω—É–∂–Ω–æ —É–±—Ä–∞—Ç—å Content-Type –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                headers = dict(self.session.headers)
                if 'Content-Type' in headers:
                    del headers['Content-Type']
                response = requests.post(
                    url,
                    files=files,
                    headers=headers,
                    timeout=60
                )
                if response.status_code == 200:
                    return response.json()
        except:
            pass
        return None

    def get_manual_search_url(self, text: str, mktu_classes: List[int] = None) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        params = {"q": text}
        if mktu_classes:
            params["classes"] = ",".join(map(str, mktu_classes))
        return f"{self.resource_info['url']}?{urllib.parse.urlencode(params)}"


class LinkmarkChecker(TrademarkChecker):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ Linkmark - –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ç–æ–≤–∞—Ä–Ω—ã–º –∑–Ω–∞–∫–∞–º –†–§
    https://linkmark.ru/

    Linkmark –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã –§–ò–ü–°/–†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç
    —É–¥–æ–±–Ω—ã–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–∏—Å–∫–∞.
    """

    def __init__(self):
        super().__init__()
        self.base_url = "https://linkmark.ru"
        self.search_url = f"{self.base_url}/search"
        self.resource_info = TRADEMARK_RESOURCES["linkmark"]

    def check_trademark(self, text: str, mktu_classes: List[int] = None) -> TrademarkCheckResult:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ —á–µ—Ä–µ–∑ Linkmark.
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–∑–≤—É—á–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π.
        """
        result = TrademarkCheckResult(
            resource_name=self.resource_info["name"],
            resource_url=self.resource_info["url"],
            search_query=text,
            mktu_classes=mktu_classes or []
        )

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_variants = TextSimilarity.transliterate_variants(text)
        print(f"[Linkmark] –ü–æ–∏—Å–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: {search_variants}")

        all_found_matches = []
        best_status = RiskLevel.GREEN
        search_notes = []

        for variant in search_variants[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 3 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            try:
                # POST –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–∏—Å–∫
                search_data = {"search": variant}

                response = self.session.post(
                    self.search_url,
                    data=search_data,
                    timeout=30,
                    allow_redirects=True
                )

                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    # –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —ç—Ç–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
                    temp_result = TrademarkCheckResult(
                        resource_name=self.resource_info["name"],
                        resource_url=self.resource_info["url"],
                        search_query=variant,
                        mktu_classes=mktu_classes or []
                    )
                    self._parse_linkmark_results(temp_result, soup, variant, mktu_classes)
                    print(f"[Linkmark] –ü–æ—Å–ª–µ _parse: temp_result.found_matches = {len(temp_result.found_matches)}")

                    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    for match in temp_result.found_matches:
                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞—Ä–∏–∞–Ω—Ç–µ –ø–æ–∏—Å–∫–∞
                        match['search_variant'] = variant
                        if match not in all_found_matches:
                            all_found_matches.append(match)

                    # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π —Å—Ç–∞—Ç—É—Å
                    if temp_result.status == RiskLevel.RED:
                        best_status = RiskLevel.RED
                        if temp_result.exact_match:
                            result.exact_match = True
                        if temp_result.similar_match:
                            result.similar_match = True
                    elif temp_result.status == RiskLevel.YELLOW and best_status != RiskLevel.RED:
                        best_status = RiskLevel.YELLOW

                    if temp_result.notes:
                        search_notes.append(f"[{variant}]: {temp_result.notes}")

                    # –û–±–Ω–æ–≤–ª—è–µ–º similarity_score
                    result.similarity_score = max(result.similarity_score, temp_result.similarity_score)

                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    if len(search_variants) > 1:
                        time.sleep(0.5)

            except requests.exceptions.RequestException as e:
                search_notes.append(f"[{variant}]: –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è")
            except Exception as e:
                search_notes.append(f"[{variant}]: –û—à–∏–±–∫–∞: {str(e)}")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"[Linkmark FINAL] all_found_matches: {len(all_found_matches)}, best_status: {best_status}")
        result.found_matches = all_found_matches[:15]  # –ú–∞–∫—Å–∏–º—É–º 15 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"[Linkmark FINAL] result.found_matches assigned: {len(result.found_matches)}")
        result.status = best_status

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –∑–∞–º–µ—Ç–∫–∏
        if len(search_variants) > 1:
            variants_info = f" (–ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã: {', '.join(search_variants[:3])})"
        else:
            variants_info = ""

        if result.exact_match:
            result.notes = f"–ù–∞–π–¥–µ–Ω —Ç–æ–∂–¥–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¢–ó!{variants_info}"
        elif result.similar_match:
            result.notes = f"–ù–∞–π–¥–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ –¢–ó{variants_info}"
        elif all_found_matches:
            result.notes = f"–ù–∞–π–¥–µ–Ω–æ {len(all_found_matches)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤{variants_info}"
        else:
            result.notes = f"–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ{variants_info}"

        if not all_found_matches and not result.notes:
            result.notes = f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ {self.base_url}"
            result.status = RiskLevel.YELLOW

        return result

    def _parse_linkmark_results(self, result: TrademarkCheckResult,
                                 soup: BeautifulSoup, search_text: str,
                                 mktu_filter: List[int] = None):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ Linkmark —Å –°–¢–†–û–ì–û–ô —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –ú–ö–¢–£.
        –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–ª–∞—Å—Å—ã –ú–ö–¢–£ - –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –¢–û–õ–¨–ö–û —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —ç—Ç–∏—Ö –∫–ª–∞—Å—Å–∞—Ö.
        """

        # –ò—â–µ–º —Å—á–µ—Ç—á–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–∞–±–∞—Ö
        total_marks = 0
        total_apps = 0

        tabs = soup.find_all('li', {'data-name': True})
        for tab in tabs:
            count_div = tab.find('div', class_='result-count')
            if count_div:
                try:
                    count = int(count_div.get_text(strip=True))
                    tab_name = tab.get('data-name', '')
                    if tab_name == 'tab-marks':
                        total_marks = count
                    elif tab_name == 'tab-apps':
                        total_apps = count
                except ValueError:
                    pass

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä –ú–ö–¢–£ –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        mktu_filter_str = set(str(c) for c in mktu_filter) if mktu_filter else None
        print(f"[Linkmark] mktu_filter={mktu_filter}, mktu_filter_str={mktu_filter_str}")

        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        items = soup.find_all('div', class_='result-div-item')
        print(f"[Linkmark] –ù–∞–π–¥–µ–Ω–æ {len(items)} –∫–∞—Ä—Ç–æ—á–µ–∫ –¢–ó –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

        # –°—á—ë—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        matches_in_mktu = 0  # –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö –ú–ö–¢–£
        matches_outside_mktu = 0  # –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤–Ω–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        high_similarity_count = 0  # –í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (>80%)

        # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_in_mktu = []  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö –ú–ö–¢–£
        results_outside_mktu = []  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–Ω–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤

        for item in items[:50]:  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞
            number_div = item.find('div', class_='result-div-item-number')
            reg_number = ""
            if number_div:
                link = number_div.find('a')
                if link:
                    reg_number = link.get_text(strip=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∞—Å—Å—ã –ú–ö–¢–£
            mktu_div = item.find('div', class_='result-div-item-mktu')
            tm_classes = []
            if mktu_div:
                mktu_text = mktu_div.get_text(strip=True)
                tm_classes = [c.strip() for c in mktu_text.split(',') if c.strip()]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–ø–∞–¥–∞–µ—Ç –ª–∏ –¢–ó –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –ú–ö–¢–£
            mktu_match = False
            if mktu_filter_str:
                mktu_match = bool(set(tm_classes) & mktu_filter_str)
            else:
                mktu_match = True  # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –Ω–µ —É–∫–∞–∑–∞–Ω, —Å—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è
            owner_div = item.find('div', class_='result-div-item-owner')
            holder = ""
            if owner_div:
                holder = owner_div.get_text(strip=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç—É—Å
            status_div = item.find('div', class_='result-div-item-status')
            tm_status = ""
            if status_div:
                status_text = status_div.find('div')
                if status_text:
                    tm_status = status_text.get_text(strip=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞
            words_div = item.find('div', class_='words-part')
            trademark_words = ""
            if words_div:
                trademark_words = words_div.get_text(strip=True)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –¶–ï–õ–´–ú –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞
            compare_text = trademark_words if trademark_words else ""
            best_score = 0.0
            best_reason = "–ù–∞–π–¥–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞"
            is_exact_name_match = False  # –§–ª–∞–≥ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –í–°–ï–ì–û –Ω–∞–∑–≤–∞–Ω–∏—è

            if compare_text:
                search_normalized = TextSimilarity.normalize_text(search_text)
                compare_normalized = TextSimilarity.normalize_text(compare_text)

                # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –í–°–ï–ì–û –Ω–∞–∑–≤–∞–Ω–∏—è
                if search_normalized == compare_normalized:
                    best_score = 1.0
                    best_reason = "–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è"
                    is_exact_name_match = True
                else:
                    # 1.1 –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
                    search_variants = TextSimilarity.transliterate_variants(search_text)
                    compare_variants = TextSimilarity.transliterate_variants(compare_text)
                    for sv in search_variants:
                        sv_norm = TextSimilarity.normalize_text(sv)
                        for cv in compare_variants:
                            cv_norm = TextSimilarity.normalize_text(cv)
                            if sv_norm == cv_norm:
                                best_score = 1.0
                                best_reason = "–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è)"
                                is_exact_name_match = True
                                break
                        if is_exact_name_match:
                            break

                if not is_exact_name_match:
                    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –í–°–ï–ì–û –Ω–∞–∑–≤–∞–Ω–∏—è
                    is_similar_full, score_full, reason_full = TextSimilarity.check_similarity(
                        search_text, compare_text, 0.7
                    )
                    if score_full > best_score:
                        best_score = score_full
                        best_reason = reason_full

                    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞—à –∑–∞–ø—Ä–æ—Å –æ–¥–Ω–∏–º –∏–∑ —Å–ª–æ–≤ –≤ –¢–ó
                    # –ù–æ —ç—Ç–æ –ù–ï —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ç–æ—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º - —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç–∏—á–Ω–æ–µ
                    words_list = compare_normalized.split()
                    if len(words_list) > 1:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –¢–ó –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤
                        for word in words_list:
                            word_similarity = TextSimilarity.levenshtein_similarity(search_normalized, word)
                            if word_similarity >= 0.9:
                                # –°–ª–æ–≤–æ –Ω–∞–π–¥–µ–Ω–æ, –Ω–æ —ç—Ç–æ —á–∞—Å—Ç—å —Å–æ—Å—Ç–∞–≤–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
                                # –°–Ω–∏–∂–∞–µ–º –æ—Ü–µ–Ω–∫—É –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª–∏–Ω–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                                partial_score = word_similarity * (len(search_normalized) / len(compare_normalized))
                                partial_score = min(partial_score, 0.7)  # –ú–∞–∫—Å–∏–º—É–º 70% –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                                if partial_score > best_score:
                                    best_score = partial_score
                                    best_reason = f"–ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Å–ª–æ–≤–æ '{word}' –≤ —Å–æ—Å—Ç–∞–≤–Ω–æ–º –Ω–∞–∑–≤–∞–Ω–∏–∏)"

                    # 4. –ï—Å–ª–∏ –Ω–∞—à –∑–∞–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω–µ–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –¢–ó –≤ –Ω–∞—à–µ–º –∑–∞–ø—Ä–æ—Å–µ
                    if search_normalized in compare_normalized or compare_normalized in search_normalized:
                        # –û–¥–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–º - –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –¥–ª–∏–Ω
                        shorter = min(len(search_normalized), len(compare_normalized))
                        longer = max(len(search_normalized), len(compare_normalized))
                        containment_score = shorter / longer
                        if containment_score > best_score and containment_score < 0.9:
                            best_score = containment_score
                            best_reason = "–ß–∞—Å—Ç–∏—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ"

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ü–û–õ–ù–û–°–¢–¨–Æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ
            is_exact = is_exact_name_match and best_score >= 0.95
            is_high_similar = best_score >= 0.8 and not is_exact_name_match
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –ø–æ–∏—Å–∫–∞ Linkmark (–æ–Ω–∏ —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É)
            # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ - —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ Linkmark
            is_relevant = bool(reg_number)  # –õ—é–±–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–æ–º–µ—Ä–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏

            # –ï—Å–ª–∏ –Ω–µ—Ç score, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if best_score == 0 and reg_number:
                best_score = 0.1
                best_reason = "–ù–∞–π–¥–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞ Linkmark"

            # –°–æ–∑–¥–∞—ë–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏
            match_info = {
                "text": trademark_words or f"–¢–ó ‚Ññ{reg_number}",
                "registration_number": reg_number,
                "similarity_score": best_score,  # 0-1 –¥–ª—è –ª–æ–≥–∏–∫–∏, —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ 100 –≤ —à–∞–±–ª–æ–Ω–µ
                "reason": best_reason,
                "classes": tm_classes,
                "classes_str": ", ".join(tm_classes) if tm_classes else "–Ω–µ —É–∫–∞–∑–∞–Ω—ã",
                "status": tm_status,
                "holder": holder[:100] if holder else "",
                "mktu_match": mktu_match
            }

            # –û—Ç–ª–∞–¥–∫–∞: –≤—ã–≤–æ–¥–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print(f"[Linkmark DEBUG] –¢–ó #{reg_number}: '{trademark_words[:40] if trademark_words else '-'}', –ú–ö–¢–£: {tm_classes}, score: {best_score:.2f}, mktu_match: {mktu_match}, is_relevant: {is_relevant}")

            # –°–¢–†–û–ì–ê–Ø —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ú–ö–¢–£:
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–∏–ª—å—Ç—Ä –ú–ö–¢–£ - –¥–æ–±–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —ç—Ç–∏—Ö –∫–ª–∞—Å—Å–∞—Ö
            if mktu_filter_str:
                if mktu_match:
                    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ –ú–ö–¢–£ - –¥–æ–±–∞–≤–ª—è–µ–º –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    if is_relevant:
                        results_in_mktu.append(match_info)
                        matches_in_mktu += 1

                        if is_exact:
                            result.exact_match = True
                        elif is_high_similar:
                            result.similar_match = True
                            high_similarity_count += 1

                        result.similarity_score = max(result.similarity_score, best_score)
                        if reg_number:
                            result.registration_numbers.append(reg_number)
                else:
                    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–Ω–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ - —Å—á–∏—Ç–∞–µ–º –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                    if is_relevant:
                        matches_outside_mktu += 1
                        results_outside_mktu.append(match_info)
            else:
                # –§–∏–ª—å—Ç—Ä –ú–ö–¢–£ –Ω–µ —É–∫–∞–∑–∞–Ω - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –ø–æ–∏—Å–∫–∞
                if is_relevant:
                    results_in_mktu.append(match_info)

                    if is_exact:
                        result.exact_match = True
                    elif is_high_similar:
                        result.similar_match = True
                        high_similarity_count += 1

                    result.similarity_score = max(result.similarity_score, best_score)
                    if reg_number:
                        result.registration_numbers.append(reg_number)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: 1) –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ (–ø–æ–ª–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–µ—Ä–≤—ã–º–∏), 2) –¥–µ–π—Å—Ç–≤—É—é—â–∏–µ –ø–µ—Ä–≤—ã–º–∏, 3) –∏—Å—Ç—ë–∫—à–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏
        def sort_key(x):
            # –°—Ö–æ–∂–µ—Å—Ç—å: –ø–æ–ª–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (>= 0.9) –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0
            similarity = x.get('similarity_score', 0)
            if similarity >= 0.9:
                similarity_priority = 0
            elif similarity >= 0.7:
                similarity_priority = 1
            else:
                similarity_priority = 2

            # –°—Ç–∞—Ç—É—Å: "–¥–µ–π—Å—Ç–≤—É–µ—Ç" = 0 (–ø–µ—Ä–≤—ã–µ), "–∏—Å—Ç—ë–∫"/"–Ω–µ –¥–µ–π—Å—Ç–≤—É–µ—Ç" = 1 (–ø–æ—Å–ª–µ–¥–Ω–∏–µ)
            status_lower = x.get('status', '').lower()
            if status_lower in ['–¥–µ–π—Å—Ç–≤—É–µ—Ç', '–¥–µ–π—Å—Ç–≤—É—é—â–∏–π']:
                status_priority = 0
            elif '–∏—Å—Ç—ë–∫' in status_lower or '–∏—Å—Ç–µ–∫' in status_lower or '–Ω–µ –¥–µ–π—Å—Ç–≤—É–µ—Ç' in status_lower:
                status_priority = 2  # –í —Å–∞–º—ã–π –∫–æ–Ω–µ—Ü
            else:
                status_priority = 1

            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å –¥–ª—è –≤—Ç–æ—Ä–∏—á–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (–∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º)
            detail_similarity = 1 - similarity

            return (similarity_priority, status_priority, detail_similarity)

        results_in_mktu.sort(key=sort_key)
        print(f"[Linkmark] results_in_mktu: {len(results_in_mktu)}, results_outside_mktu: {len(results_outside_mktu)}")
        result.found_matches = results_in_mktu[:15]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 15 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"[Linkmark] result.found_matches: {len(result.found_matches)}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å —É—á—ë—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ú–ö–¢–£
        self._set_status(result, total_marks, total_apps, matches_in_mktu,
                        matches_outside_mktu, mktu_filter)

    def _set_status(self, result: TrademarkCheckResult, total_marks: int = 0,
                    total_apps: int = 0, matches_in_mktu: int = 0,
                    matches_outside_mktu: int = 0, mktu_filter: List[int] = None):
        """
        –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        –í–ê–ñ–ù–û: –°—Ç–∞—Ç—É—Å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¢–û–õ–¨–ö–û –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö –ú–ö–¢–£.
        """

        mktu_info = f" (–∫–ª–∞—Å—Å {', '.join(map(str, mktu_filter))})" if mktu_filter else ""

        if mktu_filter:
            # –°–¢–†–û–ì–ò–ô –†–ï–ñ–ò–ú: —É–∫–∞–∑–∞–Ω—ã –∫–ª–∞—Å—Å—ã –ú–ö–¢–£
            if result.exact_match:
                result.status = RiskLevel.RED
                result.notes = f"üî¥ –ó–ê–ü–†–ï–©–ï–ù–û: –ù–∞–π–¥–µ–Ω —Ç–æ–∂–¥–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¢–ó –≤ –∫–ª–∞—Å—Å–µ{mktu_info}!"
            elif result.similar_match and matches_in_mktu > 0:
                result.status = RiskLevel.RED
                result.notes = f"üî¥ –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {matches_in_mktu} –ø–æ—Ö–æ–∂–∏—Ö –¢–ó –≤ –∫–ª–∞—Å—Å–µ{mktu_info}"
            elif matches_in_mktu > 0:
                result.status = RiskLevel.YELLOW
                result.notes = f"üü° –ù–∞–π–¥–µ–Ω–æ {matches_in_mktu} –¢–ó –≤ –∫–ª–∞—Å—Å–µ{mktu_info}. –¢—Ä–µ–±—É–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑."
            elif matches_outside_mktu > 0:
                # –ï—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –Ω–æ –≤ –¥—Ä—É–≥–∏—Ö –∫–ª–∞—Å—Å–∞—Ö - —ç—Ç–æ –ó–ï–õ–Å–ù–´–ô –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
                result.status = RiskLevel.GREEN
                result.notes = f"üü¢ –í –∫–ª–∞—Å—Å–µ{mktu_info} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ù–ï–¢. (–í –¥—Ä—É–≥–∏—Ö –∫–ª–∞—Å—Å–∞—Ö: {matches_outside_mktu} –¢–ó)"
            elif total_marks > 0:
                result.status = RiskLevel.GREEN
                result.notes = f"üü¢ –í –∫–ª–∞—Å—Å–µ{mktu_info} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ù–ï–¢. (–í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {total_marks} –ø–æ—Ö–æ–∂–∏—Ö –¢–ó –≤ –¥—Ä—É–≥–∏—Ö –∫–ª–∞—Å—Å–∞—Ö)"
            else:
                result.status = RiskLevel.GREEN
                result.notes = f"üü¢ –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –±–∞–∑–µ –¢–ó –†–§ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ{mktu_info}"
        else:
            # –ë–ï–ó –§–ò–õ–¨–¢–†–ê –ú–ö–¢–£: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å—ë
            if result.exact_match:
                result.status = RiskLevel.RED
                result.notes = f"üî¥ –ó–ê–ü–†–ï–©–ï–ù–û: –ù–∞–π–¥–µ–Ω —Ç–æ–∂–¥–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¢–ó! –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {total_marks} –¢–ó"
            elif result.similar_match:
                result.status = RiskLevel.YELLOW
                result.notes = f"üü° –ù–∞–π–¥–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ –¢–ó (–≤—Å–µ–≥–æ {total_marks}). –£–∫–∞–∂–∏—Ç–µ –∫–ª–∞—Å—Å –ú–ö–¢–£ –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏."
            elif total_marks > 0:
                result.status = RiskLevel.YELLOW
                result.notes = f"üü° –ù–∞–π–¥–µ–Ω–æ {total_marks} –¢–ó —Å –ø–æ—Ö–æ–∂–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏. –£–∫–∞–∂–∏—Ç–µ –∫–ª–∞—Å—Å –ú–ö–¢–£ –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏."
            else:
                result.status = RiskLevel.GREEN
                result.notes = f"üü¢ –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –±–∞–∑–µ –¢–ó –†–§ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"


class WIPOChecker(TrademarkChecker):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ WIPO Global Brand Database
    https://branddb.wipo.int/
    """

    def __init__(self):
        super().__init__()
        self.base_url = "https://branddb.wipo.int"
        self.api_url = f"{self.base_url}/branddb/en/similarname"
        self.resource_info = TRADEMARK_RESOURCES["wipo"]

    def check_trademark(self, text: str, mktu_classes: List[int] = None) -> TrademarkCheckResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ —á–µ—Ä–µ–∑ WIPO"""
        result = TrademarkCheckResult(
            resource_name=self.resource_info["name"],
            resource_url=self.resource_info["url"],
            search_query=text,
            mktu_classes=mktu_classes or []
        )

        try:
            # WIPO –∏–º–µ–µ—Ç —Å–ª–æ–∂–Ω—ã–π API, —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–ø—Ä–æ—Å–∞
            search_structure = {
                "boolean": "AND",
                "bricks": [
                    {
                        "key": "brandName",
                        "value": text,
                        "strategy": "Simple"
                    }
                ]
            }

            if mktu_classes:
                search_structure["bricks"].append({
                    "key": "niceClass",
                    "value": ",".join(map(str, mktu_classes)),
                    "strategy": "Simple"
                })

            params = {
                "sort": "score desc",
                "start": 0,
                "rows": 30,
                "asStructure": json.dumps(search_structure)
            }

            response = self.session.get(
                self.api_url,
                params=params,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                self._process_wipo_results(result, data, text)
            else:
                result.notes = f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ {self.base_url}"
                result.status = RiskLevel.YELLOW

        except Exception as e:
            result.notes = f"–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {str(e)}"
            result.status = RiskLevel.YELLOW

        return result

    def _process_wipo_results(self, result: TrademarkCheckResult,
                               data: Dict, search_text: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ WIPO"""
        docs = data.get("response", {}).get("docs", [])

        for doc in docs:
            brand_name = doc.get("brandName", "")

            is_similar, score, reason = TextSimilarity.check_similarity(
                search_text, brand_name, self.similarity_threshold
            )

            if is_similar:
                result.found_matches.append({
                    "text": brand_name,
                    "registration_number": doc.get("ST13", ""),
                    "similarity_score": score,
                    "reason": reason,
                    "holder": doc.get("holderName", ""),
                    "country": doc.get("designationCurrentStatusCode", ""),
                    "classes": doc.get("niceClass", [])
                })

                if score == 1.0:
                    result.exact_match = True
                else:
                    result.similar_match = True

                result.similarity_score = max(result.similarity_score, score)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if result.exact_match:
            result.status = RiskLevel.RED
            result.notes = "–ù–∞–π–¥–µ–Ω —Ç–æ–∂–¥–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π —Ç–æ–≤–∞—Ä–Ω—ã–π –∑–Ω–∞–∫"
        elif result.similar_match:
            result.status = RiskLevel.YELLOW
            result.notes = "–ù–∞–π–¥–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Ç–æ–≤–∞—Ä–Ω—ã–µ –∑–Ω–∞–∫–∏"
        else:
            result.status = RiskLevel.GREEN
            result.notes = "–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π –±–∞–∑–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

    def get_manual_search_url(self, text: str) -> str:
        """URL –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        encoded = urllib.parse.quote(text)
        return f"{self.base_url}/branddb/en/?q=brandName:{encoded}"


class ComprehensiveTrademarkChecker:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –ø–æ –≤—Å–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º –±–∞–∑–∞–º
    """

    def __init__(self):
        self.checkers = {
            "linkmark": LinkmarkChecker()
        }

    def check_all(self, text: str, mktu_classes: List[int] = None,
                  check_international: bool = True) -> List[TrademarkCheckResult]:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –≤—Å–µ–º –±–∞–∑–∞–º

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            mktu_classes: –ö–ª–∞—Å—Å—ã –ú–ö–¢–£
            check_international: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –±–∞–∑—ã
        """
        results = []

        # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –±–∞–∑—ã
        linkmark_result = self.checkers["linkmark"].check_trademark(text, mktu_classes)
        results.append(linkmark_result)

        return results

    def get_overall_status(self, results: List[TrademarkCheckResult]) -> Tuple[RiskLevel, str]:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        has_red = any(r.status == RiskLevel.RED for r in results)
        has_yellow = any(r.status == RiskLevel.YELLOW for r in results)

        if has_red:
            return RiskLevel.RED, "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Ç–æ–≤–∞—Ä–Ω—ã–º–∏ –∑–Ω–∞–∫–∞–º–∏"
        elif has_yellow:
            return RiskLevel.YELLOW, "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤"
        else:
            return RiskLevel.GREEN, "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –Ω–µ –≤—ã—è–≤–∏–ª–∞ –ø—Ä–æ–±–ª–µ–º"

    def generate_manual_check_links(self, text: str, mktu_classes: List[int] = None) -> Dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        links = {}

        # –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞
        rospatent_url = TRADEMARK_RESOURCES["rospatent_platform"]["url"]
        if mktu_classes:
            params = {"q": text, "classes": ",".join(map(str, mktu_classes))}
        else:
            params = {"q": text}
        links["–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞"] = f"{rospatent_url}?{urllib.parse.urlencode(params)}"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ (Linkmark)
        links["–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞"] = f"{TRADEMARK_RESOURCES['linkmark']['url']}?search={urllib.parse.quote(text)}"

        # WIPO Global Brand Database (–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –±–∞–∑–∞)
        wipo_params = {"brandName": text}
        if mktu_classes:
            wipo_params["niceClass"] = ",".join(map(str, mktu_classes))
        links["WIPO Global Brand Database"] = f"https://branddb.wipo.int/en/quicksearch/brand?{urllib.parse.urlencode(wipo_params)}"

        return links


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    checker = ComprehensiveTrademarkChecker()

    # –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
    test_text = "EXAMPLE BRAND"
    test_classes = [25, 35]

    print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–Ω–∞–∫–∞: {test_text}")
    print(f"–ö–ª–∞—Å—Å—ã –ú–ö–¢–£: {test_classes}")
    print("-" * 50)

    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    links = checker.generate_manual_check_links(test_text, test_classes)
    print("\n–°—Å—ã–ª–∫–∏ –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏:")
    for name, url in links.items():
        print(f"  {name}: {url}")
